{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nooyod0924/python-ml/blob/main/10%2CTransfer_Learning_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cifar10 이미지](https://pytorch.org/tutorials/_images/cifar10.png)\n",
        "![ImageNet의 루트-리프 가지 두 개(포유류 하위 트리 및 차량 하위 트리)의 스냅샷입니다](https://devopedia.org/images/article/172/7316.1561043304.png)\n",
        "\n",
        "# <font color='red'><b>Transfer Learning (전이학습)</b></font>\n",
        "- 고해상도의 컬러 이미지를 학습하기 위해 실무에서는 처음부터 CNN 아키텍처를 구축하고 가중치나 바이어스등을 학습시키는 것이아니라,\n",
        "- <font color='red'><b>고해상도 컬러 이미지에 잘 훈련된 사전학습(pre-trained)된 CNN 모델이 있다면,</b></font>\n",
        "  - 이러한 CNN 모델을 바탕으로 우리가 분석하고자 하는 이미지 데이터에 맞도록,\n",
        "  - <font color='red'><b>이미 학습되어 있는 CNN 모델의 다양한 파라미터 등을 수정(tuning)해서 사용</b></font>함으로서,\n",
        "  - 임의의 값으로 초기화된 파라미터을 처음부터 학습시키는 것에 비해 소요시간을 획기적으로 줄일 수 있으며 다양한 이미지 데이터를 짧은 시간에 학습 할 수 있는 장정이 있다.\n",
        "- 고해상도 컬러 이미지 특성을 파악하는데 있어서 최고의 성능을 나타내고 소스까지 공개되어 있는 Google Inception 모델이나 MS ResNet등을 이용하고 있는데.\n",
        "- <font color='red'><b>이처럼 우리가 분석하고자 하는 데이터에 맞도록 미세한 조정, 즉 Fine Tuning으로 불리는 작은 변화만을 주어 학습을 시키는 방법을 Transfer Learning(전이학습)이라고 함.</b></font>\n",
        "\n",
        "[심층 합성곱 신경망을 사용한 ImageNet 분류](https://blog.acolyer.org/2016/04/20/imagenet-classification-with-deep-convolutional-neural-networks/)"
      ],
      "metadata": {
        "id": "8HMg5sWzoKbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Inception-V3를 이용한 실습 \n"
      ],
      "metadata": {
        "id": "4-xX1cV4wIku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# 이미지 경로 설정\n",
        "daisy_path = 'daisy.jpg'\n",
        "dandelion_path = 'dandelion.jpg'\n",
        "rose_path = 'rose.jpg'\n",
        "sunflower_path = 'sunflower.jpg'\n",
        "tulip_path = 'tulip.jpg'\n",
        "\n",
        "# 이미지 불러오기\n",
        "daisy_img = load_img(daisy_path, target_size=(299, 299))\n",
        "dandelion_img = load_img(dandelion_path, target_size=(299, 299))\n",
        "rose_img = load_img(rose_path, target_size=(299, 299))\n",
        "sunflower_img = load_img(sunflower_path, target_size=(299, 299))\n",
        "tulip_img = load_img(tulip_path, target_size=(299, 299))\n",
        "\n",
        "# 이미지를 배열로 변환\n",
        "daisy_arr = img_to_array(daisy_img)\n",
        "dandelion_arr = img_to_array(dandelion_img)\n",
        "rose_arr = img_to_array(rose_img)\n",
        "sunflower_arr = img_to_array(sunflower_img)\n",
        "tulip_arr = img_to_array(tulip_img)\n",
        "\n",
        "# 모델 로드\n",
        "model = tf.keras.applications.InceptionV3()\n",
        "\n",
        "# 이미지 전처리\n",
        "daisy_arr = tf.keras.applications.inception_v3.preprocess_input(daisy_arr)\n",
        "dandelion_arr = tf.keras.applications.inception_v3.preprocess_input(dandelion_arr)\n",
        "rose_arr = tf.keras.applications.inception_v3.preprocess_input(rose_arr)\n",
        "sunflower_arr = tf.keras.applications.inception_v3.preprocess_input(sunflower_arr)\n",
        "tulip_arr = tf.keras.applications.inception_v3.preprocess_input(tulip_arr)\n",
        "\n",
        "# 이미지를 입력으로 모델 실행\n",
        "daisy_pred = model.predict(np.array([daisy_arr]))\n",
        "dandelion_pred = model.predict(np.array([dandelion_arr]))\n",
        "rose_pred = model.predict(np.array([rose_arr]))\n",
        "sunflower_pred = model.predict(np.array([sunflower_arr]))\n",
        "tulip_pred = model.predict(np.array([tulip_arr]))\n",
        "\n",
        "# 분류 결과 출력\n",
        "daisy_label = tf.keras.applications.inception_v3.decode_predictions(daisy_pred)[0][0][1]\n",
        "dandelion_label = tf.keras.applications.inception_v3.decode_predictions(dandelion_pred)[0][0][1]\n",
        "rose_label = tf.keras.applications.inception_v3.decode_predictions(rose_pred)[0][0][1]\n",
        "sunflower_label = tf.keras.applications.inception_v3.decode_predictions(sunflower_pred)[0][0][1]\n",
        "tulip_label = tf.keras.applications.inception_v3.decode_predictions(tulip_pred)[0][0][1]\n",
        "\n",
        "print(f'Daisy: {daisy_label}')\n",
        "print(f'Dandelion: {dandelion_label}')\n",
        "print(f'Rose: {rose_label}')\n",
        "print(f'Sunflower: {sunflower_label}')\n",
        "print(f'Tulip: {tulip_label}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "OWVG2XxdpVRX",
        "outputId": "b335dd34-7fce-41fb-ca10-928f6dd0a1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-36f25aac6641>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 이미지 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdaisy_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaisy_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdandelion_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdandelion_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrose_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrose_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'daisy.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfsEikF85KMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}